{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de4b21c-3f79-4fbe-a559-232fc0b1a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def cdiv(a,b):\n",
    "    return (a+b-1) //b\n",
    "    \n",
    "import torch.nn as nn\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import gc\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def kernel1(\n",
    "    x_ptr, l1_ptr, xl1_ptr, x_quant_ptr, scale_ptr,\n",
    "    m, n, k,\n",
    "    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, group_sz: tl.constexpr, bk_quant: tl.constexpr, group_size: tl.constexpr\n",
    "):\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "    pid_m_new, pid_n_new = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)\n",
    "\n",
    "    # Matmul: X @ l1\n",
    "    acc = tl.zeros((bm, bn), dtype=tl.float32)\n",
    "    x_ptr_offset = k * (pid_m_new * bm + tl.expand_dims(tl.arange(0, bm), 1)) + tl.expand_dims(tl.arange(0, bk), 0)\n",
    "    l1_ptr_offset = (pid_n_new * bn + tl.expand_dims(tl.arange(0, bn), 0)) + tl.expand_dims(tl.arange(0, bk), 1) * n\n",
    "    x_ptr_mask = k * tl.expand_dims(pid_m_new * bm + tl.arange(1, bm + 1), 1)\n",
    "    x_ptr_mask = tl.where(x_ptr_mask < m * k, x_ptr_mask, m * k)\n",
    "    l1_mask = (k - 1) * n + pid_n_new * bn + tl.expand_dims(tl.arange(1, bn + 1), 0)\n",
    "    l1_mask = tl.where(l1_mask < k * n, l1_mask, k * n)\n",
    "    for i in range(0, tl.cdiv(k, bk)):\n",
    "        x_loaded = tl.load(x_ptr + x_ptr_offset, x_ptr_offset < x_ptr_mask, 0)\n",
    "        l1_loaded = tl.load(l1_ptr + l1_ptr_offset, l1_ptr_offset < l1_mask, 0)\n",
    "        acc += tl.dot(x_loaded, l1_loaded)\n",
    "        x_ptr_offset += bk\n",
    "        l1_ptr_offset += n * bk\n",
    "    xl1_offset = n * (pid_m_new * bm + tl.expand_dims(tl.arange(0, bm), 1)) + pid_n_new * bn + tl.expand_dims(tl.arange(0, bn), 0)\n",
    "    xl1_mask = n * (pid_m_new * bm + tl.expand_dims(tl.arange(0, bm), 1) + 1)\n",
    "    acc = acc.to(tl.float16)\n",
    "    tl.store(xl1_ptr + xl1_offset, acc, xl1_offset < xl1_mask)\n",
    "\n",
    "    if pid_n_new==0:\n",
    "        xptr_offset_1=k *(bm*pid_m_new + tl.expand_dims(tl.arange(0, bm),1)) + 2*tl.expand_dims(tl.arange(0, group_size), 0)\n",
    "        xptr_offset_2=k *(bm*pid_m_new + tl.expand_dims(tl.arange(0, bm),1)) + 2*tl.expand_dims(tl.arange(0, group_size), 0)+1\n",
    "        x_ptr_mask = k * tl.expand_dims(pid_m_new * bm + tl.arange(1, bm + 1), 1)\n",
    "        x_ptr_mask=tl.where(x_ptr_mask<m*k , x_ptr_mask , m*k)\n",
    "\n",
    "        \n",
    "        x_save_offset=k//2 *(bm*pid_m_new + tl.expand_dims(tl.arange(0, bm),1)) + tl.expand_dims(tl.arange(0, group_size), 0)\n",
    "        x_save_mask = k//2 * tl.expand_dims(pid_m_new * bm + tl.arange(1, bm + 1), 1)\n",
    "        x_save_mask=tl.where(x_save_mask<m*k//2 , x_ptr_mask , m*k//2)\n",
    "        \n",
    "        x_scale_offset=tl.cdiv(k,2*group_size)*(bm*pid_m_new + tl.expand_dims(tl.arange(0, bm),1))\n",
    "        x_scale_offset_mask=tl.cdiv(k,2*group_size)*(bm*pid_m_new + tl.expand_dims(tl.arange(1, bm+1),1))\n",
    "        x_scale_offset_mask=tl.where(x_scale_offset_mask<m*tl.cdiv(k,group_size) , x_ptr_mask , m*tl.cdiv(k,group_size))\n",
    "\n",
    "        for i in range(0, tl.cdiv(k, 2*group_size)):\n",
    "            x_loaded_1=tl.load(x_ptr+xptr_offset_1 ,xptr_offset_1<x_ptr_mask ,0)\n",
    "            x_loaded_2=tl.load(x_ptr+xptr_offset_2 ,xptr_offset_2<x_ptr_mask ,0)\n",
    "            \n",
    "            max_val_1=tl.max(tl.abs(x_loaded_1) , axis=1 , keep_dims=True)\n",
    "            max_val_2=tl.max(tl.abs(x_loaded_2) , axis=1 , keep_dims=True)\n",
    "            max_val=tl.where(max_val_1>max_val_2 , max_val_1 , max_val_2)\n",
    "            scale = max_val/7.0 \n",
    "            scaled_1 = x_loaded_1 / scale\n",
    "            scaled_2 = x_loaded_2 / scale\n",
    "            clamped_vals_1 = tl.clamp(scaled_1, -8.0, 7.0)\n",
    "            clamped_vals_2 = tl.clamp(scaled_2, -8.0, 7.0)\n",
    "            int4_vals_1 = tl.where(clamped_vals_1 >= 0,\n",
    "                                 tl.floor(clamped_vals_1 + 0.5),\n",
    "                                 tl.ceil(clamped_vals_1 - 0.5)).to(tl.int8)\n",
    "                \n",
    "            int4_vals_2 = tl.where(clamped_vals_2 >= 0,\n",
    "                                 tl.floor(clamped_vals_2 + 0.5),\n",
    "                                 tl.ceil(clamped_vals_2 - 0.5)).to(tl.int8)\n",
    "            int8block = int4_vals_1 & 0x0F  # Lower 4 bits\n",
    "            int8block2 = int4_vals_2 & 0x0F  # Upper 4 bits\n",
    "            packed = (int8block2.to(tl.int8) << 4) | int8block.to(tl.int8)\n",
    "            packed=packed.to(tl.int8)\n",
    "            tl.store(x_quant_ptr +x_save_offset,packed  , x_save_offset<x_save_mask)\n",
    "            tl.store(scale_ptr+x_scale_offset ,scale ,  x_scale_offset<x_scale_offset_mask)\n",
    "    \n",
    "            xptr_offset_1+=2*group_size\n",
    "            xptr_offset_2+=2*group_size\n",
    "            x_save_offset+=group_size\n",
    "            x_scale_offset+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61cb23fa-16a9-40eb-a190-1d271d1fcddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_vertical_pytorch(tensor, quant_group_size):\n",
    "    \"\"\"\n",
    "    Quantizes a tensor into a vertically tiled format compatible with dequantize_vertical_pytorch.\n",
    "    tensor: (k, n) float tensor to quantize\n",
    "    m: unused (kept for signature compatibility)\n",
    "    n: number of columns\n",
    "    quant_group_size: size of quantization groups along k dimension\n",
    "    Returns:\n",
    "        tensor_quant: (k//2, n) quantized tensor (uint8, two 4-bit values per byte)\n",
    "        scale: (k//quant_group_size, n) scaling factors\n",
    "    \"\"\"\n",
    "    k,n = tensor.shape\n",
    "    assert k % 2 == 0, \"k must be even for vertical quantization\"\n",
    "    assert k % quant_group_size == 0, \"k must be divisible by quant_group_size\"\n",
    "\n",
    "    # Compute scales per group\n",
    "    num_groups = k // quant_group_size\n",
    "    tensor_groups = tensor.reshape(num_groups, quant_group_size, n)\n",
    "    scale = torch.max(torch.abs(tensor_groups), dim=1)[0] / 7.0  # Max value for int4 is 7\n",
    "    scale = scale.clamp(min=1e-6)  # Avoid division by zero\n",
    "\n",
    "    # Expand scales to match tensor shape\n",
    "    scale_expanded = torch.repeat_interleave(scale, quant_group_size, dim=0)\n",
    "\n",
    "    # Quantize to int8 range (-8 to 7)\n",
    "    tensor_scaled = tensor / scale_expanded\n",
    "    tensor_int8 = torch.round(tensor_scaled.clamp(-8, 7)).to(torch.int8)\n",
    "\n",
    "    # Convert to uint4 range (0-15: -8 to -1 becomes 8-15, 0-7 stays 0-7)\n",
    "    tensor_uint4 = torch.where(tensor_int8 < 0, tensor_int8 + 16, tensor_int8).to(torch.int8)\n",
    "\n",
    "    # Pack two 4-bit values into one byte\n",
    "    tensor_uint4 = tensor_uint4.reshape(k // 2, 2, n)  # Group pairs along k\n",
    "    low_bits = tensor_uint4[:, 0, :] & 0x0F\n",
    "    high_bits = (tensor_uint4[:, 1, :] & 0x0F) << 4\n",
    "    tensor_quant = (high_bits | low_bits).to(torch.int8)\n",
    "\n",
    "    return tensor_quant, scale , tensor.shape\n",
    "\n",
    "def quantize_horizontal_pytorch(tensor, quant_group_size):\n",
    "    \"\"\"\n",
    "    Quantizes a tensor into a horizontally tiled format compatible with dequantize_horizontal_pytorch.\n",
    "    tensor: (m, k) float tensor to quantize\n",
    "    m: number of rows\n",
    "    k: number of columns\n",
    "    quant_group_size: size of quantization groups along k dimension\n",
    "    Returns:\n",
    "        tensor_quant: (m, k//2) quantized tensor (uint8, two 4-bit values per byte)\n",
    "        scale: (m, k//quant_group_size) scaling factors\n",
    "    \"\"\"\n",
    "    m,k = tensor.shape\n",
    "    assert k % 2 == 0, \"k must be even for horizontal quantization\"\n",
    "    assert k % quant_group_size == 0, \"k must be divisible by quant_group_size\"\n",
    "\n",
    "    # Compute scales per group\n",
    "    num_groups = k // quant_group_size\n",
    "    tensor_groups = tensor.reshape(m, num_groups, quant_group_size)\n",
    "    scale = torch.max(torch.abs(tensor_groups), dim=2)[0] / 7.0  # Max value for int4 is 7\n",
    "    scale = scale.clamp(min=1e-6)  # Avoid division by zero\n",
    "\n",
    "    # Expand scales to match tensor shape\n",
    "    scale_expanded = torch.repeat_interleave(scale, quant_group_size, dim=1)\n",
    "\n",
    "    # Quantize to int8 range (-8 to 7)\n",
    "    tensor_scaled = tensor / scale_expanded\n",
    "    tensor_int8 = torch.round(tensor_scaled.clamp(-8, 7)).to(torch.int8)\n",
    "\n",
    "    # Convert to uint4 range (0-15: -8 to -1 becomes 8-15, 0-7 stays 0-7)\n",
    "    tensor_uint4 = torch.where(tensor_int8 < 0, tensor_int8 + 16, tensor_int8).to(torch.int8)\n",
    "\n",
    "    # Pack two 4-bit values into one byte\n",
    "    tensor_uint4 = tensor_uint4.reshape(m, k // 2, 2)  # Group pairs along k\n",
    "    low_bits = tensor_uint4[:, :, 0] & 0x0F\n",
    "    high_bits = (tensor_uint4[:, :, 1] & 0x0F) << 4\n",
    "    tensor_quant = (high_bits | low_bits).to(torch.int8)\n",
    "\n",
    "    return tensor_quant, scale , tensor.shape\n",
    "\n",
    "\n",
    "def dequantize_vertical_pytorch(tensor_quant, m, n, scale, quant_group_size):\n",
    "    \"\"\"\n",
    "    Dequantizes a vertically tiled quantized tensor similar to dequant_vertical.\n",
    "    tensor_quant: (k//2, n) quantized tensor (uint8, two 4-bit values per byte)\n",
    "    scale: (k//quant_group_size, n) scaling factors\n",
    "    \"\"\"\n",
    "    k = 2 * tensor_quant.shape[0]  # Full dimension after unpacking\n",
    "    # Convert to int16 to prevent overflow during bit operations\n",
    "    tensor_quant_int16 = tensor_quant.to(torch.int16)\n",
    "    \n",
    "    # Unpack all 4-bit values at once\n",
    "    # Reshape to (k//2, n, 1) and expand to separate high and low bits\n",
    "    low_bits = tensor_quant_int16 & 0x0F  # Lower 4 bits\n",
    "    high_bits = (tensor_quant_int16 >> 4) & 0x0F  # Upper 4 bits\n",
    "    \n",
    "    # Stack and reshape: interleave low and high bits\n",
    "    tensor = torch.stack([low_bits, high_bits], dim=1).reshape(k, n)\n",
    "    \n",
    "    # Convert to signed int8 (0-7 stay as is, 8-15 become -8 to -1)\n",
    "    tensor = torch.where(tensor < 8, tensor, tensor - 16).to(torch.int8)\n",
    "    \n",
    "    # Apply scales\n",
    "    scale_expanded = torch.repeat_interleave(scale, quant_group_size, dim=0)[:k]\n",
    "    tensor = tensor.to(torch.float16) * scale_expanded\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def dequantize_horizontal_pytorch(tensor_quant, m, k, scale, quant_group_size):\n",
    "    \"\"\"\n",
    "    Dequantizes a horizontally tiled quantized tensor similar to dequant_horizontal.\n",
    "    tensor_quant: (m, k//2) quantized tensor (uint8, two 4-bit values per byte)\n",
    "    scale: (m, k//quant_group_size) scaling factors\n",
    "    \"\"\"\n",
    "    # Convert to int16 to prevent overflow during bit operations\n",
    "    tensor_quant_int16 = tensor_quant.to(torch.int16)\n",
    "    \n",
    "    # Unpack all 4-bit values at once\n",
    "    low_bits = tensor_quant_int16 & 0x0F  # Lower 4 bits\n",
    "    high_bits = (tensor_quant_int16 >> 4) & 0x0F  # Upper 4 bits\n",
    "    \n",
    "    # Stack and reshape: interleave low and high bits\n",
    "    tensor = torch.stack([low_bits, high_bits], dim=2).reshape(m, k)\n",
    "    \n",
    "    # Convert to signed int8\n",
    "    tensor = torch.where(tensor < 8, tensor, tensor - 16).to(torch.int8)\n",
    "    \n",
    "    # Apply scales\n",
    "    scale_expanded = torch.repeat_interleave(scale, quant_group_size, dim=1)[:, :k]\n",
    "    tensor = tensor.to(torch.float16) * scale_expanded\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9804be94-e098-41a3-ac42-00771120012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@triton.jit\n",
    "def dequant_vertical(\n",
    "    tensor_ptr,\n",
    "    bn,bk,\n",
    "    stride,\n",
    "    tensor_startpoint,\n",
    "    m,n , \n",
    "    scale_ptr,\n",
    "    scale_start,\n",
    "    scale_group_size,\n",
    "):\n",
    "    offset_tensor = tensor_startpoint + tl.expand_dims(tl.arange(0,bn) , 0)+(stride)*(tl.expand_dims(tl.arange(0,bk) , 1)//2)\n",
    "    offset_tensor_mask=tensor_startpoint+(n)*((tl.expand_dims(tl.arange(0,bk) , 1))//2+1)\n",
    "    offset_tensor_mask=tl.where(offset_tensor_mask<m*n//2 ,offset_tensor_mask , m*n//2 )\n",
    "    \n",
    "\n",
    "    # print(offset_tensor , offset_tensor<offset_tensor_mask)\n",
    "    offset_scale=scale_start + tl.expand_dims(tl.arange(0,bn) , 0)\n",
    "    offset_scale_mask=scale_start + min(bn,n)\n",
    "    \n",
    "    tensor=tl.load(tensor_ptr+offset_tensor  ,offset_tensor<offset_tensor_mask , 0)\n",
    "    scale=tl.load(scale_ptr+offset_scale  ,offset_scale<offset_scale_mask , 0)\n",
    "    \n",
    "    shifter=(0*tl.expand_dims(tl.arange(0,bn) , 0)+(tl.expand_dims(tl.arange(0,bk) , 1))%2)*4\n",
    "    tensor = (tensor >> shifter) & 0x0F \n",
    "    tensor=tl.where(tensor<8 , tensor , tensor-16).to(tl.int8)\n",
    "    # print(\"vertical\" , tensor , scale)\n",
    "    return tensor,scale\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def dequant_horizontal(\n",
    "    tensor_ptr,\n",
    "    bm , bk,\n",
    "    stride,\n",
    "    tensor_startpoint,\n",
    "    m,n,\n",
    "    scale_ptr,\n",
    "    scale_start,\n",
    "    scale_group_size\n",
    "):\n",
    "    offset_tensor=tensor_startpoint+(stride//2)*(tl.expand_dims(tl.arange(0,bm) , 1)) + tl.expand_dims(tl.arange(0,bk), 0)//2\n",
    "    offset_tensor_mask=tensor_startpoint+stride//2*(tl.expand_dims(tl.arange(1,bm+1) , 1))\n",
    "    offset_tensor_mask=tl.where(offset_tensor_mask<m*n//2 ,offset_tensor_mask , m*n//2 )\n",
    "    \n",
    "    offset_scale=scale_start+(tl.cdiv(stride,scale_group_size)*(tl.expand_dims(tl.arange(0,bm) , 1)))\n",
    "    offset_scale_mask=scale_start+(tl.cdiv(stride,scale_group_size)*(tl.expand_dims(tl.arange(1,bm+1) , 1)))\n",
    "    offset_scale_mask=tl.where(offset_scale_mask<m*tl.cdiv(stride,scale_group_size) ,offset_scale_mask , m*(tl.cdiv(stride,scale_group_size) ))\n",
    "    \n",
    "    # print(offset_scale , offset_scale_mask)\n",
    "    tensor=tl.load(tensor_ptr+offset_tensor  ,offset_tensor<offset_tensor_mask , 0)\n",
    "    scale=tl.load(scale_ptr+offset_scale  ,offset_scale<offset_scale_mask, 0)\n",
    "    shift_offset=bk*(tl.expand_dims(tl.arange(0,bm) , 1)) + tl.expand_dims(tl.arange(0,bk), 0)\n",
    "    shifter = (shift_offset % 2) * 4\n",
    "    \n",
    "    tensor = (tensor >> shifter) & 0x0F \n",
    "    tensor=tl.where(tensor<8 , tensor , tensor-16).to(tl.int8)\n",
    "    # print(\"horizontal\" ,  tensor , scale)\n",
    "    return tensor , scale\n",
    "\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def kernel2(\n",
    "    xl1_ptr , x1_quant_ptr , x1_scale_ptr , w_quant_ptr , w_scale_ptr , l2_ptr , output_ptr,bias_ptr,\n",
    "    m,n,k ,r,\n",
    "    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, group_sz: tl.constexpr , quant_group_size: tl.constexpr\n",
    "):\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "    pid_m_new , pid_n_new=tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  # Weirdness: tl.swizzle2d doesn't work when simulating on CPU\n",
    "    \n",
    "    acc = tl.zeros((bm, bn), dtype=tl.float32)\n",
    "    \n",
    "    \n",
    "    # x_ptr_offset=k*(pid_m_new*bm + tl.expand_dims(tl.arange(0,bm), 1))+ tl.expand_dims(tl.arange(0,bk), 0)\n",
    "    # x_ptr_mask=k*tl.expand_dims(pid_m_new*bm+tl.arange(1,bm+1),1)\n",
    "    # x_ptr_mask=tl.where(x_ptr_mask<m*k , x_ptr_mask , m*k)\n",
    "    # x_ptr_scale_offset=k*(pid_m_new*bm + tl.expand_dims(tl.arange(0,bm), 1))+ tl.expand_dims(tl.arange(0,bk), 0)\n",
    "    \n",
    "    # w_ptr_offset=(pid_n_new*bn+tl.expand_dims(tl.arange(0,bn), 0))+tl.expand_dims(tl.arange(0,bk), 1)*n\n",
    "    # w_mask=(k-1)*n+pid_n_new*bn+tl.expand_dims(tl.arange(1,bn+1), 0)\n",
    "    # w_mask=tl.where(l1_mask<k*n , l1_mask , k*n)\n",
    "\n",
    "    xl1_ptr_offset=r*(pid_m_new*bm + tl.expand_dims(tl.arange(0,bm), 1))+ tl.expand_dims(tl.arange(0,bk), 0)\n",
    "    x_l1_mask=r*tl.expand_dims(pid_m_new*bm+tl.arange(1,bm+1),1)\n",
    "    x_l1_mask=tl.where(x_l1_mask<m*r , x_l1_mask , m*r)\n",
    "    \n",
    "    l2_offset=(pid_n_new*bn+tl.expand_dims(tl.arange(0,bn), 0))+tl.expand_dims(tl.arange(0,bk), 1)*n\n",
    "    l2_mask=pid_n_new*bn+tl.expand_dims(tl.arange(1,bk+1), 1)*n\n",
    "    l2_mask=tl.where(l2_mask<r*n , l2_mask , r*n)\n",
    "\n",
    "    # print(l2_offset)\n",
    "    startpoint_x=k//2*(pid_m_new*bm)\n",
    "    startpoint_x_scale=pid_m_new*bm*tl.cdiv(k,quant_group_size)\n",
    "    \n",
    "    startpoint_w=pid_n_new*bn\n",
    "    startpoint_w_scale=pid_n_new*bn\n",
    "    \n",
    "    acc = tl.zeros((bm, bn), dtype=tl.float32)\n",
    "    zeros = tl.zeros((bm, bn), dtype=tl.float32)\n",
    "    # print('pid_m_new' , pid_m_new,'pid_n_new',pid_n_new)\n",
    "    # print(\"w_start\" , startpoint_w )\n",
    "    # print(\"startpoint_w_scale\" , startpoint_w_scale )\n",
    "    # print(\"x_start\" , startpoint_x)\n",
    "    for i in range ( 0 , tl.cdiv(k , bk)):\n",
    "        \n",
    "        dequant_x1 , scale_x1=dequant_horizontal(x1_quant_ptr ,  bm , bk , k ,startpoint_x , m , k ,x1_scale_ptr , startpoint_x_scale+(i*bk)//quant_group_size ,  quant_group_size)\n",
    "        \n",
    "        dequant_w , scale_w=dequant_vertical(w_quant_ptr ,  bk , bn , n ,startpoint_w , k , n , w_scale_ptr ,startpoint_w_scale+n*((i*bk)//quant_group_size) , quant_group_size)\n",
    "    \n",
    "        acc_x_w=  tl.dot(dequant_x1, dequant_w)\n",
    "        # print(acc_x_w)\n",
    "        acc_x_scale_w_scale = scale_x1*scale_w\n",
    "        startpoint_x+=bk//2\n",
    "        startpoint_w+=bk*n//2\n",
    "        if i*bk<r:\n",
    "            # print(xl1_ptr_offset , xl1_ptr_offset<x_l1_mask)\n",
    "            xl1_loaded = tl.load(xl1_ptr+xl1_ptr_offset , xl1_ptr_offset<x_l1_mask , 0)\n",
    "            l2_loaded = tl.load(l2_ptr+l2_offset , l2_offset<l2_mask , 0)\n",
    "            # print(xl1_loaded , l2_loaded)\n",
    "            acc_lora=  tl.dot(xl1_loaded, l2_loaded)\n",
    "            acc+=acc_lora\n",
    "            xl1_ptr_offset+=bk\n",
    "            l2_offset+=n*bk\n",
    "            l2_mask+=n*bk\n",
    "            l2_mask=tl.where(l2_mask<r*n , l2_mask , r*n)\n",
    "        acc+=tl.fma(acc_x_w , acc_x_scale_w_scale , zeros)\n",
    "    \n",
    "    answer_offset=n*(pid_m_new*bm + tl.expand_dims(tl.arange(0,bm), 1))+ pid_n_new*bn+tl.expand_dims(tl.arange(0,bn), 0)\n",
    "    answer_mask=n*(pid_m_new*bm + tl.expand_dims(tl.arange(0,bm), 1)+1)\n",
    "    answer_mask=tl.where(answer_mask<m*n , answer_mask , m*n)\n",
    "\n",
    "    bias_offset=pid_n_new*bn+tl.arange(0,bn)\n",
    "    bias=tl.load(bias_ptr+bias_offset , bias_offset<n , 0)\n",
    "    acc+=bias\n",
    "    acc=acc.to(tl.float16)\n",
    "    \n",
    "    tl.store(output_ptr+answer_offset , acc , answer_offset<answer_mask)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9495af8a-385e-46de-b872-605320de2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "\n",
    "class SDV_FC_4b(nn.Module):\n",
    "    def __init__(self, l1, l2, bias, remainder, block_size, q_bit=4):\n",
    "        \"\"\"\n",
    "        Initializes the model.\n",
    "        \n",
    "        Args:\n",
    "            l1, l2, bias: Standard parameters.\n",
    "            remainder: The tensor to be quantized in 4-bit.\n",
    "            block_size: The block size used for per-block quantization.\n",
    "            q_bit: Should be 4.\n",
    "        \"\"\"\n",
    "        super(SDV_FC_4b, self).__init__()\n",
    "        self.l1 = nn.Parameter(l1.clone()).contiguous()\n",
    "        self.l2 = nn.Parameter(l2.clone()).contiguous()\n",
    "        self.bias = nn.Parameter(bias.clone()).contiguous()\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        if q_bit != 4:\n",
    "            raise ValueError(\"This model is set up for 4-bit quantization only.\")\n",
    "        \n",
    "        # Quantize the remainder using 4-bit quantization.\n",
    "        # print(remainder.shape)\n",
    "        quantized_remainder, scales, original_shape = quantize_vertical_pytorch(remainder, block_size)\n",
    "        # print(quantized_remainder.shape , scales.shape  , original_shape)\n",
    "        # Save the packed quantized tensor and scales as buffers.\n",
    "        self.register_buffer('quantized_remainder', quantized_remainder.contiguous())\n",
    "        self.register_buffer('scales', scales.contiguous())\n",
    "        # Save the original shape as an attribute (for dequantization)\n",
    "        self.original_shape = original_shape\n",
    "        \n",
    "        # Optionally delete the original tensors to free memory.\n",
    "        del l1, l2, bias, remainder\n",
    "        self.bm, self.bn, self.bk = 64, 64, 64  # Tile sizes\n",
    "        self.group_sz = 64  # For swizzling\n",
    "    def forward(self, x):\n",
    "        # Compute the standard low-rank output.\n",
    "        x=x.contiguous()\n",
    "        # print(x.shape)\n",
    "        b,m, k = x.shape\n",
    "        rank , n =self.l2.shape\n",
    "        xl1 = torch.empty((b,m, rank), dtype=torch.float16, device=x.device)\n",
    "        q_x = torch.empty((b,m, k // 2), dtype=torch.int8, device=x.device)\n",
    "        s_x = torch.empty((b,m, k // self.block_size), dtype=torch.float16, device=x.device)\n",
    "        output = torch.empty((b,m, n), dtype=torch.float16, device=x.device)\n",
    "        \n",
    "        # Kernel 1\n",
    "        grid = lambda meta: (triton.cdiv(m, meta['bm']), triton.cdiv(n, meta['bn']))\n",
    "        kernel1[grid](\n",
    "            x, self.l1.contiguous(), xl1, q_x, s_x,\n",
    "            m, rank, k,\n",
    "            self.bm, self.bn, self.bk, self.group_sz, self.bk, self.block_size//2\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        grid = (triton.cdiv(m, self.bm), triton.cdiv(n, self.bn))\n",
    "        kernel2[grid](\n",
    "            xl1, q_x, s_x, self.quantized_remainder, self.scales, self.l2, output, self.bias,\n",
    "            m, n, k, rank,\n",
    "            self.bm, self.bn, self.bk, self.group_sz, self.block_size\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def SDV_FC_FullyConnectedLayers(fc_weight, fc_bias, block_size=64 , q_bit=4,rank=None):\n",
    "    # Compute SVD\n",
    "    U, S, Vh = torch.linalg.svd(fc_weight.float(), full_matrices=False)\n",
    "    \n",
    "    # Verify S is sorted\n",
    "    assert torch.all(S[:-1] >= S[1:]), \"Singular values not sorted!\"\n",
    "    \n",
    "    # Take components corresponding to non-zero singular values\n",
    "    U_r = U[:, :rank].to(torch.float16)\n",
    "    S_r = S[:rank].to(torch.float16)\n",
    "    V_r = Vh[:rank, :].to(torch.float16)\n",
    "    \n",
    "    # Create scaled matrices\n",
    "    l1 = U_r @ torch.diag(S_r)\n",
    "    l2 = V_r\n",
    "    remainder = fc_weight-l1 @ l2\n",
    "    del U, S, Vh, fc_weight\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache\n",
    "    gc.collect()  # Force garbage collection\n",
    "    return SDV_FC_4b(l1, l2, fc_bias, remainder , block_size , q_bit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5384ca2b-8dd9-4c07-862b-bfec1cb01003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference output shape: torch.Size([1, 32, 640])\n",
      "Original SDV-4b output shape: torch.Size([1, 32, 640])\n",
      "\n",
      "Speed Measurements (average over 100 runs on GPU):\n",
      "Reference Linear - GPU Time: 0.000029 seconds\n",
      "SDV-4b Model - GPU Time: 0.000143 seconds\n",
      "\n",
      "Accuracy Metrics:\n",
      "Mean Absolute Error (MAE): 0.058990\n",
      "Mean Squared Error (MSE): 0.005459\n",
      "Max Absolute Error: 0.326904\n",
      "\n",
      "Sample comparison:\n",
      "Reference [0, 0:5]: tensor([[ 1.0361,  0.7236,  0.7441, -0.4814, -0.9780],\n",
      "        [ 0.5396,  0.2257,  0.7158, -0.7246, -0.3179],\n",
      "        [ 0.0241, -1.1875,  0.0714, -0.2864,  0.8516],\n",
      "        [-0.3613,  0.1407, -0.0070,  0.8945,  0.4673],\n",
      "        [-0.1952, -0.7837,  0.5728,  0.1598, -0.5684]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "Original SDV-4b [0, 0:5]: tensor([[ 1.2021,  0.6494,  0.7358, -0.5649, -0.8188],\n",
      "        [ 0.5454,  0.3259,  0.6655, -0.6533, -0.2183],\n",
      "        [-0.1691, -1.2363,  0.0573, -0.3591,  0.9111],\n",
      "        [-0.2720,  0.0597, -0.0581,  0.8047,  0.4563],\n",
      "        [-0.3347, -0.8496,  0.6621,  0.2216, -0.5547]], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming SDV_FC_FullyConnectedLayers is defined elsewhere\n",
    "# from your_module import SDV_FC_FullyConnectedLayers\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if GPU is available\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"This script requires a GPU with CUDA support.\")\n",
    "    \n",
    "    # Create a linear layer on GPU\n",
    "    in_features, out_features = 1280, 640\n",
    "    # torch.manual_seed(42)\n",
    "    linear = nn.Linear(in_features, out_features, bias=True, dtype=torch.float16).cuda()\n",
    "    \n",
    "    # Convert to original SDV_FC_4b on GPU\n",
    "    block_size = 64\n",
    "    rank = 32\n",
    "    svd_model = SDV_FC_FullyConnectedLayers(linear.weight.t(), linear.bias, block_size=block_size, rank=rank).cuda()\n",
    "    \n",
    "    # Input on GPU\n",
    "    batch_size = 32\n",
    "    X = torch.randn((1, batch_size, in_features), dtype=torch.float16).cuda()\n",
    "    \n",
    "    # Warm-up runs (to avoid initial CUDA overhead)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = linear(X)\n",
    "            _ = svd_model(X)\n",
    "    \n",
    "    # GPU timing using CUDA events\n",
    "    num_iterations = 100\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    # Time reference model\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            output_ref = linear(X)\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "    ref_time = start_event.elapsed_time(end_event) / (num_iterations * 1000)  # Convert to seconds\n",
    "    \n",
    "    # Time SVD model\n",
    "    torch.cuda.synchronize()\n",
    "    start_event.record()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            output_svd = svd_model(X)\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "    svd_time = start_event.elapsed_time(end_event) / (num_iterations * 1000)  # Convert to seconds\n",
    "    \n",
    "    # Compute accuracy metrics\n",
    "    with torch.no_grad():\n",
    "        output_ref = linear(X)\n",
    "        output_svd = svd_model(X)\n",
    "    \n",
    "    mae = torch.mean(torch.abs(output_ref - output_svd)).item()\n",
    "    mse = torch.mean((output_ref - output_svd) ** 2).item()\n",
    "    max_error = torch.max(torch.abs(output_ref - output_svd)).item()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Reference output shape: {output_ref.shape}\")\n",
    "    print(f\"Original SDV-4b output shape: {output_svd.shape}\")\n",
    "    print(f\"\\nSpeed Measurements (average over {num_iterations} runs on GPU):\")\n",
    "    print(f\"Reference Linear - GPU Time: {ref_time:.6f} seconds\")\n",
    "    print(f\"SDV-4b Model - GPU Time: {svd_time:.6f} seconds\")\n",
    "    print(f\"\\nAccuracy Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.6f}\")\n",
    "    print(f\"Max Absolute Error: {max_error:.6f}\")\n",
    "    print(\"\\nSample comparison:\")\n",
    "    print(\"Reference [0, 0:5]:\", output_ref[0,0:5, 0:5])\n",
    "    print(\"Original SDV-4b [0, 0:5]:\", output_svd[0,0:5, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fdfdbdd-9631-4e18-befa-9a63a4ff397e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quantize_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquantize_x\u001b[49m[\u001b[38;5;241m0\u001b[39m] , q_x [\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'quantize_x' is not defined"
     ]
    }
   ],
   "source": [
    "quantize_x[0] , q_x [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49857b3c-fcb5-41ea-88bd-0de759d4e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantised_x_scale[0], s_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7ea73a-339b-4943-88f6-d4f52e57baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9c160-37cd-4278-b9f2-b2bd4bb1789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantised_x_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73448f8-ade3-45ed-abea-12606733ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ref[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9485f2fb-8f63-4e38-affd-9716a9589fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_svd[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
