{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5dfa80-be11-42fd-8f2f-a5d67346cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['TRITON_INTERPRET'] = '1' \n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def tryfunc():\n",
    "    x1 = torch.randn(160, dtype=torch.float32).cuda()  # Generate in float32 on GPU\n",
    "    x1 = (x1 * 25.5 + 127.5).clamp(0, 255).to(torch.uint8).contiguous()  # Scale to [0, 255] and convert\n",
    "    grid = lambda meta: (triton.cdiv(100, meta['bm'])  , )\n",
    "    batchsize=16\n",
    "    halfbatchsize=8\n",
    "    print(x1)\n",
    "    exp[grid](x1 , bm=batchsize , bmh=halfbatchsize)\n",
    "@triton.jit\n",
    "def exp(\n",
    "    xl1_ptr,\n",
    "    bm:tl.constexpr,bmh:tl.constexpr,\n",
    "):\n",
    "    # print(bm)\n",
    "    offs_k = tl.arange(0, bm)\n",
    "    offs_k=offs_k//2\n",
    "    print(offs_k)\n",
    "    off_k_half=tl.arange(0, bm)\n",
    "    shifter = (off_k_half % 2) * 4\n",
    "    b=tl.load(xl1_ptr +offs_k , offs_k<100 ,0)\n",
    "    print(shifter[None, :])\n",
    "    print(b)\n",
    "    b = (b >> shifter[None:,]) & 0x0F \n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9743dd-d68a-4ceb-af78-5c8532dc4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def dequant_vertical(\n",
    "    tensor_ptr,\n",
    "    bn,bk,\n",
    "    stride,\n",
    "    tensor_startpoint,\n",
    "    m,n , \n",
    "    scale_ptr,\n",
    "    scale_start,\n",
    "    scale_group_size,\n",
    "):\n",
    "    offset_tensor = tensor_startpoint + tl.expand_dims(tl.arange(0,bn) , 0)+(stride)*(tl.expand_dims(tl.arange(0,bk) , 1)//2)\n",
    "    offset_tensor_mask=tensor_startpoint+(n)*((tl.expand_dims(tl.arange(0,bk) , 1))//2+1)\n",
    "    offset_tensor_mask=tl.where(offset_tensor_mask<m*n//2 ,offset_tensor_mask , m*n//2 )\n",
    "    \n",
    "\n",
    "    # print(offset_tensor , offset_tensor<offset_tensor_mask)\n",
    "    offset_scale=scale_start + tl.expand_dims(tl.arange(0,bn) , 0)\n",
    "    offset_scale_mask=scale_start + min(bn,n)\n",
    "    \n",
    "    tensor=tl.load(tensor_ptr+offset_tensor  ,offset_tensor<offset_tensor_mask , 0)\n",
    "    scale=tl.load(scale_ptr+offset_scale  ,offset_scale<offset_scale_mask , 0)\n",
    "    \n",
    "    shifter=(0*tl.expand_dims(tl.arange(0,bn) , 0)+(tl.expand_dims(tl.arange(0,bk) , 1))%2)*4\n",
    "    tensor = (tensor >> shifter) & 0x0F \n",
    "    tensor=tl.where(tensor<8 , tensor , tensor-16).to(tl.int8)\n",
    "    # print(\"vertical\" , tensor , scale)\n",
    "    return tensor,scale\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def dequant_horizontal(\n",
    "    tensor_ptr,\n",
    "    bm , bk,\n",
    "    stride,\n",
    "    tensor_startpoint,\n",
    "    m,n,\n",
    "    scale_ptr,\n",
    "    scale_start,\n",
    "    scale_group_size\n",
    "):\n",
    "    offset_tensor=tensor_startpoint+(stride//2)*(tl.expand_dims(tl.arange(0,bm) , 1)) + tl.expand_dims(tl.arange(0,bk), 0)//2\n",
    "    offset_tensor_mask=tensor_startpoint+stride//2*(tl.expand_dims(tl.arange(1,bm+1) , 1))\n",
    "    offset_tensor_mask=tl.where(offset_tensor_mask<m*n//2 ,offset_tensor_mask , m*n//2 )\n",
    "    \n",
    "    offset_scale=scale_start+(tl.cdiv(stride,scale_group_size)*(tl.expand_dims(tl.arange(0,bm) , 1)))\n",
    "    offset_scale_mask=scale_start+(tl.cdiv(stride,scale_group_size)*(tl.expand_dims(tl.arange(1,bm+1) , 1)))\n",
    "    offset_scale_mask=tl.where(offset_scale_mask<m*tl.cdiv(stride,scale_group_size) ,offset_scale_mask , m*(tl.cdiv(stride,scale_group_size) ))\n",
    "    \n",
    "    # print(offset_scale , offset_scale_mask)\n",
    "    tensor=tl.load(tensor_ptr+offset_tensor  ,offset_tensor<offset_tensor_mask , 0)\n",
    "    scale=tl.load(scale_ptr+offset_scale  ,offset_scale<offset_scale_mask, 0)\n",
    "    shift_offset=bk*(tl.expand_dims(tl.arange(0,bm) , 1)) + tl.expand_dims(tl.arange(0,bk), 0)\n",
    "    shifter = (shift_offset % 2) * 4\n",
    "    \n",
    "    tensor = (tensor >> shifter) & 0x0F \n",
    "    tensor=tl.where(tensor<8 , tensor , tensor-16).to(tl.int8)\n",
    "    # print(\"horizontal\" ,  tensor , scale)\n",
    "    return tensor , scale\n",
    "\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def grouped_matmul_k(\n",
    "    xl1_ptr , x1_quant_ptr , x1_scale_ptr , w_quant_ptr , w_scale_ptr , l2_ptr , output_ptr,\n",
    "    m,n,k ,r,\n",
    "    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, group_sz: tl.constexpr , quant_group_size: tl.constexpr\n",
    "):\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "    pid_m_new , pid_n_new=tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  # Weirdness: tl.swizzle2d doesn't work when simulating on CPU\n",
    "    \n",
    "    acc = tl.zeros((bm, bn), dtype=tl.float32)\n",
    "    \n",
    "    \n",
    "    # x_ptr_offset=k*(pid_m_new*bm + tl.expand_dims(tl.arange(0,bm), 1))+ tl.expand_dims(tl.arange(0,bk), 0)\n",
    "    # x_ptr_mask=k*tl.expand_dims(pid_m_new*bm+tl.arange(1,bm+1),1)\n",
    "    # x_ptr_mask=tl.where(x_ptr_mask<m*k , x_ptr_mask , m*k)\n",
    "    # x_ptr_scale_offset=k*(pid_m_new*bm + tl.expand_dims(tl.arange(0,bm), 1))+ tl.expand_dims(tl.arange(0,bk), 0)\n",
    "    \n",
    "    # w_ptr_offset=(pid_n_new*bn+tl.expand_dims(tl.arange(0,bn), 0))+tl.expand_dims(tl.arange(0,bk), 1)*n\n",
    "    # w_mask=(k-1)*n+pid_n_new*bn+tl.expand_dims(tl.arange(1,bn+1), 0)\n",
    "    # w_mask=tl.where(l1_mask<k*n , l1_mask , k*n)\n",
    "\n",
    "    xl1_ptr_offset=r*(pid_m_new*bm + tl.expand_dims(tl.arange(0,bm), 1))+ tl.expand_dims(tl.arange(0,bk), 0)\n",
    "    x_l1_mask=r*tl.expand_dims(pid_m_new*bm+tl.arange(1,bm+1),1)\n",
    "    x_l1_mask=tl.where(x_l1_mask<m*r , x_l1_mask , m*r)\n",
    "    \n",
    "    l2_offset=(pid_n_new*bn+tl.expand_dims(tl.arange(0,bn), 0))+tl.expand_dims(tl.arange(0,bk), 1)*n\n",
    "    l2_mask=pid_n_new*bn+tl.expand_dims(tl.arange(1,bk+1), 1)*n\n",
    "    l2_mask=tl.where(l2_mask<r*n , l2_mask , r*n)\n",
    "\n",
    "    # print(l2_offset)\n",
    "    startpoint_x=k//2*(pid_m_new*bm)\n",
    "    startpoint_x_scale=pid_m_new*bm*tl.cdiv(k,quant_group_size)\n",
    "    \n",
    "    startpoint_w=pid_n_new*bn\n",
    "    startpoint_w_scale=pid_n_new*bn\n",
    "    \n",
    "    acc = tl.zeros((bm, bn), dtype=tl.float32)\n",
    "    zeros = tl.zeros((bm, bn), dtype=tl.float32)\n",
    "    # print('pid_m_new' , pid_m_new,'pid_n_new',pid_n_new)\n",
    "    # print(\"w_start\" , startpoint_w )\n",
    "    # print(\"startpoint_w_scale\" , startpoint_w_scale )\n",
    "    # print(\"x_start\" , startpoint_x)\n",
    "    for i in range ( 0 , tl.cdiv(k , bk)):\n",
    "        \n",
    "        dequant_x1 , scale_x1=dequant_horizontal(x1_quant_ptr ,  bm , bk , k ,startpoint_x , m , k ,x1_scale_ptr , startpoint_x_scale+(i*bk)//quant_group_size ,  quant_group_size)\n",
    "        \n",
    "        dequant_w , scale_w=dequant_vertical(w_quant_ptr ,  bk , bn , n ,startpoint_w , k , n , w_scale_ptr ,startpoint_w_scale+n*((i*bk)//quant_group_size) , quant_group_size)\n",
    "    \n",
    "        acc_x_w=  tl.dot(dequant_x1, dequant_w)\n",
    "        # print(acc_x_w)\n",
    "        acc_x_scale_w_scale = scale_x1*scale_w\n",
    "        startpoint_x+=bk//2\n",
    "        startpoint_w+=bk*n//2\n",
    "        if i*bk<r:\n",
    "            # print(xl1_ptr_offset , xl1_ptr_offset<x_l1_mask)\n",
    "            xl1_loaded = tl.load(xl1_ptr+xl1_ptr_offset , xl1_ptr_offset<x_l1_mask , 0)\n",
    "            l2_loaded = tl.load(l2_ptr+l2_offset , l2_offset<l2_mask , 0)\n",
    "            # print(xl1_loaded , l2_loaded)\n",
    "            acc_lora=  tl.dot(xl1_loaded, l2_loaded)\n",
    "            acc+=acc_lora\n",
    "            xl1_ptr_offset+=bk\n",
    "            l2_offset+=n*bk\n",
    "            l2_mask+=n*bk\n",
    "            l2_mask=tl.where(l2_mask<r*n , l2_mask , r*n)\n",
    "        acc+=tl.fma(acc_x_w , acc_x_scale_w_scale , zeros)\n",
    "    \n",
    "    answer_offset=n*(pid_m_new*bm + tl.expand_dims(tl.arange(0,bm), 1))+ pid_n_new*bn+tl.expand_dims(tl.arange(0,bn), 0)\n",
    "    answer_mask=n*(pid_m_new*bm + tl.expand_dims(tl.arange(0,bm), 1)+1)\n",
    "    answer_mask=tl.where(answer_mask<m*n , answer_mask , m*n)\n",
    "    tl.store(output_ptr+answer_offset , acc , answer_offset<answer_mask)    \n",
    "    # acc=acc.to(tl.float16)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf8cc27-bccb-4f04-955c-d73db68a9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdiv(a,b):\n",
    "    return (a+b-1) //b\n",
    "def matmul(x1l1, x1Quant  ,x1scale , WQuant , Wscale , l2 ):\n",
    "    m=x1l1.shape[0]\n",
    "    n=l2.shape[1]\n",
    "    k=2*x1Quant.shape[1]\n",
    "    r=l2.shape[0]\n",
    "    answer = torch.zeros((m, n), dtype=torch.float16).cuda().contiguous()\n",
    "    batch_size=64\n",
    "    quant_group_size=64\n",
    "    \n",
    "    grid = lambda meta: (triton.cdiv(m, meta['bm']),  triton.cdiv(n, meta['bn']))\n",
    "\n",
    "    grouped_matmul_k[grid](\n",
    "        xl1_ptr=x1l1 , x1_quant_ptr=x1Quant , x1_scale_ptr=x1scale , w_quant_ptr=WQuant , w_scale_ptr=Wscale , l2_ptr=l2 , \n",
    "    m=m,n=n,k=k ,r=r,\n",
    "    bm= batch_size, bn= batch_size, bk= batch_size, group_sz= batch_size , quant_group_size= quant_group_size , output_ptr=answer\n",
    "    )\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97917cfc-7d91-4efd-b550-7987cd5d09f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=124\n",
    "k& 0x0F,(k >> 4) & 0x0F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6a658b-2ef7-47cd-a231-054a5b7dcadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   4.1484,    8.5312, -115.3125,  -15.0391],\n",
       "        [   2.5039,   -2.8086,   -5.7070,   -6.1797],\n",
       "        [   0.3042,   16.4219,  -58.0625,    4.8828],\n",
       "        [   5.3203,  -35.6250,    5.3086,  -14.5234],\n",
       "        [  -7.0742,   33.8750,   14.9609,   10.7109],\n",
       "        [   3.2148,    1.0303,  -11.8828,   14.8047],\n",
       "        [  14.2656,   67.1250,   -7.7500,   47.1250],\n",
       "        [  11.6406,  -23.3594,  -68.6875,   -2.5684],\n",
       "        [  -6.9727,   39.6562,   -5.5664,   15.5547],\n",
       "        [   7.0273,   14.2109,    7.5547,   14.8594]], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,k_half,n,r=10 , 4 , 4,4\n",
    "\n",
    "x1_quant = torch.randn((m,k_half), dtype=torch.float16).cuda()  # Generate in float32 on GPU\n",
    "x1_quant = (x1_quant * 25.5 + 127.5).clamp(0, 255).to(torch.uint8).contiguous()  # Scale to [0, 255] and convert\n",
    "x1_scale = torch.randn((m,cdiv(2*k_half,64)), dtype=torch.float16).cuda().contiguous()  # Generate in float32 on GPU\n",
    "\n",
    "w_quant = torch.randn((k_half , n), dtype=torch.float16).cuda()  # Generate in float32 on GPU\n",
    "w_quant = (w_quant * 25.5 + 127.5).clamp(0, 255).to(torch.uint8).contiguous()  # Scale to [0, 255] and convert\n",
    "w_scale = torch.randn((cdiv(2*k_half,64) , n), dtype=torch.float16).cuda().contiguous()  # Generate in float32 on GPU\n",
    "\n",
    "x1l1 = torch.randn((m , r), dtype=torch.float16).cuda().contiguous()  # Generate in float32 on GPU\n",
    "l2 = torch.randn((r , n), dtype=torch.float16).cuda().contiguous()  # Generate in float32 on GPU\n",
    "\n",
    "matmul(x1l1 , x1_quant , x1_scale , w_quant , w_scale , l2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b2a0ca-11ba-4e16-a52d-c9f278cf6c1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (16) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [10, 16].  Tensor sizes: [10, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m even_Values \u001b[38;5;241m=\u001b[39m x1_quant \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0x0F\u001b[39m  \u001b[38;5;66;03m# 0 , 2 , 4 , 8 ..\u001b[39;00m\n\u001b[1;32m     27\u001b[0m odd_Values \u001b[38;5;241m=\u001b[39m (x1_quant \u001b[38;5;241m>>\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0x0F\u001b[39m  \u001b[38;5;66;03m# 1 , 2 , 3 , 4 ....\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m x1quant\u001b[38;5;241m=\u001b[39m\u001b[43mmerge_even_odd\u001b[49m\u001b[43m(\u001b[49m\u001b[43meven_Values\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43modd_Values\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint8)\n\u001b[1;32m     29\u001b[0m x1quant\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mwhere(x1quant\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m8\u001b[39m , x1quant , x1quant\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     30\u001b[0m expanded_scale\u001b[38;5;241m=\u001b[39mexpand_scale(x1_scale , m , \u001b[38;5;241m32\u001b[39m , \u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mmerge_even_odd\u001b[0;34m(even_Values, odd_Values, m, k)\u001b[0m\n\u001b[1;32m      4\u001b[0m dtype \u001b[38;5;241m=\u001b[39m even_Values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m      7\u001b[0m merged \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((m, k), dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmerged\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m even_Values  \u001b[38;5;66;03m# Even indices\u001b[39;00m\n\u001b[1;32m     12\u001b[0m merged[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m odd_Values   \u001b[38;5;66;03m# Odd indices\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merged\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (16) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [10, 16].  Tensor sizes: [10, 4]"
     ]
    }
   ],
   "source": [
    "def merge_even_odd(even_Values, odd_Values, m, k):\n",
    "    \n",
    "    device = even_Values.device\n",
    "    dtype = even_Values.dtype\n",
    "    \n",
    "    \n",
    "    merged = torch.zeros((m, k), dtype=dtype, device=device)\n",
    "    \n",
    "    \n",
    "    merged[:, 0::2] = even_Values  # Even indices\n",
    "    \n",
    "    merged[:, 1::2] = odd_Values   # Odd indices\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def expand_scale(x1_quant_scale, m, k, group_size):\n",
    "\n",
    "    num_groups = x1_quant_scale.shape[1]  # k//(2*group_size)\n",
    "    # Reshape to (m, num_groups, 1) and repeat across 2*group_size elements\n",
    "    scale_expanded = x1_quant_scale.unsqueeze(-1).repeat(1, 1, 2 * group_size)\n",
    "    # Flatten to (m, k), trimming excess if k is not perfectly divisible\n",
    "    scale_expanded = scale_expanded.reshape(m, -1)[:, :k]\n",
    "    return scale_expanded\n",
    "    \n",
    "\n",
    "even_Values = x1_quant & 0x0F  # 0 , 2 , 4 , 8 ..\n",
    "odd_Values = (x1_quant >> 4) & 0x0F  # 1 , 2 , 3 , 4 ....\n",
    "x1quant=merge_even_odd(even_Values , odd_Values , m , 32) .to(torch.int8)\n",
    "x1quant=torch.where(x1quant<8 , x1quant , x1quant-16)\n",
    "expanded_scale=expand_scale(x1_scale , m , 32 , 32)\n",
    "print(x1quant , expanded_scale)\n",
    "# x1quant=x1quant * (expanded_scale  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01f7afeb-7675-46ba-a1fa-dc9fdab3227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dimensions: m=1000, n=100, k=1024, r=32\n",
      "PyTorch average time: 0.000530 seconds\n",
      "Triton average time: 0.000212 seconds\n",
      "Speedup (PyTorch/Triton): 2.50x\n",
      "Max absolute difference: 1.892578125\n",
      "Mean absolute difference: 0.09210830181837082\n",
      "Max relative difference: 0.04272996261715889\n",
      "Mean relative difference: 0.00018081805319525301\n",
      "Max magnitude: 4421.56201171875\n",
      "Mean magnitude: 524.3851928710938\n",
      "Point with max absolute difference: (548, 66)\n",
      "PyTorch value at this point: 4165.892578125\n",
      "Triton value at this point: 4164.0\n",
      "Results match within tolerance!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def dequantize_vertical_pytorch(tensor_quant, m, n, scale, quant_group_size):\n",
    "    \"\"\"\n",
    "    Dequantizes a vertically tiled quantized tensor similar to dequant_vertical.\n",
    "    tensor_quant: (k//2, n) quantized tensor (uint8, two 4-bit values per byte)\n",
    "    scale: (k//quant_group_size, n) scaling factors\n",
    "    \"\"\"\n",
    "    k = 2 * tensor_quant.shape[0]  # Full dimension after unpacking\n",
    "    # Convert to int16 to prevent overflow during bit operations\n",
    "    tensor_quant_int16 = tensor_quant.to(torch.int16)\n",
    "    \n",
    "    # Unpack all 4-bit values at once\n",
    "    # Reshape to (k//2, n, 1) and expand to separate high and low bits\n",
    "    low_bits = tensor_quant_int16 & 0x0F  # Lower 4 bits\n",
    "    high_bits = (tensor_quant_int16 >> 4) & 0x0F  # Upper 4 bits\n",
    "    \n",
    "    # Stack and reshape: interleave low and high bits\n",
    "    tensor = torch.stack([low_bits, high_bits], dim=1).reshape(k, n)\n",
    "    \n",
    "    # Convert to signed int8 (0-7 stay as is, 8-15 become -8 to -1)\n",
    "    tensor = torch.where(tensor < 8, tensor, tensor - 16).to(torch.int8)\n",
    "    \n",
    "    # Apply scales\n",
    "    scale_expanded = torch.repeat_interleave(scale, quant_group_size, dim=0)[:k]\n",
    "    tensor = tensor.to(torch.float32) * scale_expanded\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def dequantize_horizontal_pytorch(tensor_quant, m, k, scale, quant_group_size):\n",
    "    \"\"\"\n",
    "    Dequantizes a horizontally tiled quantized tensor similar to dequant_horizontal.\n",
    "    tensor_quant: (m, k//2) quantized tensor (uint8, two 4-bit values per byte)\n",
    "    scale: (m, k//quant_group_size) scaling factors\n",
    "    \"\"\"\n",
    "    # Convert to int16 to prevent overflow during bit operations\n",
    "    tensor_quant_int16 = tensor_quant.to(torch.int16)\n",
    "    \n",
    "    # Unpack all 4-bit values at once\n",
    "    low_bits = tensor_quant_int16 & 0x0F  # Lower 4 bits\n",
    "    high_bits = (tensor_quant_int16 >> 4) & 0x0F  # Upper 4 bits\n",
    "    \n",
    "    # Stack and reshape: interleave low and high bits\n",
    "    tensor = torch.stack([low_bits, high_bits], dim=2).reshape(m, k)\n",
    "    \n",
    "    # Convert to signed int8\n",
    "    tensor = torch.where(tensor < 8, tensor, tensor - 16).to(torch.int8)\n",
    "    \n",
    "    # Apply scales\n",
    "    scale_expanded = torch.repeat_interleave(scale, quant_group_size, dim=1)[:, :k]\n",
    "    tensor = tensor.to(torch.float32) * scale_expanded\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def matmul_pytorch(x1l1, x1Quant, x1scale, WQuant, Wscale, l2):\n",
    "    \"\"\"\n",
    "    PyTorch equivalent of the grouped_matmul_k kernel.\n",
    "    \"\"\"\n",
    "    m, r = x1l1.shape\n",
    "    k = 2 * x1Quant.shape[1]  # Full k dimension after dequantization\n",
    "    n = l2.shape[1]\n",
    "    quant_group_size = 64  # Match your Triton's quant_group_size\n",
    "    \n",
    "    # Dequantize x1Quant (m x k//2) -> (m x k)\n",
    "    x1_dequant = dequantize_horizontal_pytorch(x1Quant, m, k, x1scale, quant_group_size)\n",
    "    # Dequantize WQuant (k//2 x n) -> (k x n)\n",
    "    w_dequant = dequantize_vertical_pytorch(WQuant, k, n, Wscale, quant_group_size)\n",
    "\n",
    "    # Perform matrix multiplication: x1 @ W\n",
    "    result = x1_dequant @ w_dequant\n",
    "    \n",
    "    # Add LoRA contribution: xl1 @ l2\n",
    "    # print(x1l1 , l2)\n",
    "    lora = x1l1 @ l2\n",
    "    result += lora\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Your Triton matmul function (unchanged)\n",
    "def matmul_triton(x1l1, x1Quant, x1scale, WQuant, Wscale, l2):\n",
    "    m = x1l1.shape[0]\n",
    "    n = l2.shape[1]\n",
    "    k = 2 * x1Quant.shape[1]\n",
    "    r = l2.shape[0]\n",
    "    answer = torch.zeros((m, n), dtype=torch.float16, device='cuda').contiguous()\n",
    "    batch_size = 32\n",
    "    quant_group_size = 64\n",
    "    \n",
    "    grid = lambda meta: (triton.cdiv(m, meta['bm']), triton.cdiv(n, meta['bn']))\n",
    "    grouped_matmul_k[grid](\n",
    "        xl1_ptr=x1l1, x1_quant_ptr=x1Quant, x1_scale_ptr=x1scale, w_quant_ptr=WQuant,\n",
    "        w_scale_ptr=Wscale, l2_ptr=l2, output_ptr=answer,\n",
    "        m=m, n=n, k=k, r=r,\n",
    "        bm=batch_size, bn=batch_size, bk=batch_size, group_sz=batch_size,\n",
    "        quant_group_size=quant_group_size\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "# Test and compare\n",
    "def test_accuracy():\n",
    "    # Generate sample inputs\n",
    "    # m = random.randrange(50, 201, 2)  # Range: 50 to 200, step 2 (even numbers)\n",
    "    # n = random.randrange(50, 201, 2)  # Range: 50 to 200, step 2\n",
    "    # k = random.randrange(50, 201, 2)  # Range: 50 to 200, step 2\n",
    "    # r = random.randrange(2, k, 2)     # Range: 2 to k-1, step 2, ensures r < k\n",
    "    m,n,k,r=1000 , 100 , 1024 , 32\n",
    "    print(f\"Generated dimensions: m={m}, n={n}, k={k}, r={r}\")\n",
    "    quant_group_size = 64\n",
    "    \n",
    "    x1l1 = torch.randn(m, r, dtype=torch.float16, device='cuda').contiguous()\n",
    "    x1Quant = torch.randint(0, 256, (m, k//2), dtype=torch.uint8, device='cuda').contiguous()  # 0-255 for uint8\n",
    "    x1scale = torch.randn(m, cdiv(k,quant_group_size), dtype=torch.float32, device='cuda').contiguous()\n",
    "    WQuant = torch.randint(0, 256, (k//2, n), dtype=torch.uint8, device='cuda').contiguous()\n",
    "    Wscale = torch.randn(cdiv(k,quant_group_size), n, dtype=torch.float32, device='cuda').contiguous()\n",
    "    l2 = torch.randn(r, n, dtype=torch.float16, device='cuda').contiguous()\n",
    "\n",
    "    for _ in range(5):\n",
    "        matmul_pytorch(x1l1, x1Quant, x1scale, WQuant, Wscale, l2)\n",
    "        matmul_triton(x1l1, x1Quant, x1scale, WQuant, Wscale, l2)\n",
    "    torch.cuda.synchronize()  # Ensure warm-up is complete\n",
    "\n",
    "    # Time PyTorch\n",
    "    pytorch_times = []\n",
    "    for _ in range(10):  # Run 10 iterations for better averaging\n",
    "        start = time.time()\n",
    "        pytorch_result = matmul_pytorch(x1l1, x1Quant, x1scale, WQuant, Wscale, l2)\n",
    "        torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "        end = time.time()\n",
    "        pytorch_times.append(end - start)\n",
    "    torch.cuda.synchronize()\n",
    "    pytorch_avg_time = sum(pytorch_times) / len(pytorch_times)\n",
    "    \n",
    "    # Time Triton\n",
    "    triton_times = []\n",
    "    for _ in range(10):  # Run 10 iterations for better averaging\n",
    "        start = time.time()\n",
    "        triton_result = matmul_triton(x1l1, x1Quant, x1scale, WQuant, Wscale, l2)\n",
    "        torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "        end = time.time()\n",
    "        triton_times.append(end - start)\n",
    "    torch.cuda.synchronize()\n",
    "    triton_avg_time = sum(triton_times) / len(triton_times)\n",
    "    \n",
    "    # Print timing results\n",
    "    print(f\"PyTorch average time: {pytorch_avg_time:.6f} seconds\")\n",
    "    print(f\"Triton average time: {triton_avg_time:.6f} seconds\")\n",
    "    speedup = pytorch_avg_time / triton_avg_time if triton_avg_time > 0 else float('inf')\n",
    "    print(f\"Speedup (PyTorch/Triton): {speedup:.2f}x\")\n",
    "\n",
    "    \n",
    "    # Compute with PyTorch\n",
    "    pytorch_result = matmul_pytorch(x1l1, x1Quant, x1scale, WQuant, Wscale, l2)\n",
    "   \n",
    "    # Compute with Triton\n",
    "    triton_result = matmul_triton(x1l1, x1Quant, x1scale, WQuant, Wscale, l2)\n",
    "   \n",
    "    # Compare with relative tolerance\n",
    "    diff = torch.abs(pytorch_result - triton_result.to(torch.float32))\n",
    "    max_diff = diff.max().item()\n",
    "    mean_diff = diff.mean().item()\n",
    "    \n",
    "    # Compute relative difference\n",
    "    magnitude = torch.max(torch.abs(pytorch_result), torch.abs(triton_result.to(torch.float32)))\n",
    "    \n",
    "    relative_diff = diff / (magnitude + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "    max_relative_diff = relative_diff.max().item()\n",
    "    mean_relative_diff = relative_diff.mean().item()\n",
    "    \n",
    "    # Find the coordinates of the maximum absolute difference\n",
    "    max_diff_index = torch.argmax(diff)  # Get the flat index of the max difference\n",
    "    coords = torch.unravel_index(max_diff_index, diff.shape)  # Convert to coordinates\n",
    "    max_diff_point = tuple(coord.item() for coord in coords)  # Convert to tuple of integers\n",
    "    \n",
    "    # Print results including the problematic point\n",
    "    print(f\"Max absolute difference: {max_diff}\")\n",
    "    print(f\"Mean absolute difference: {mean_diff}\")\n",
    "    print(f\"Max relative difference: {max_relative_diff}\")\n",
    "    print(f\"Mean relative difference: {mean_relative_diff}\")\n",
    "    print(f\"Max magnitude: {magnitude.max().item()}\")\n",
    "    print(f\"Mean magnitude: {magnitude.mean().item()}\")\n",
    "    print(f\"Point with max absolute difference: {max_diff_point}\")\n",
    "    print(f\"PyTorch value at this point: {pytorch_result[max_diff_point].item()}\")\n",
    "    print(f\"Triton value at this point: {triton_result[max_diff_point].item()}\")\n",
    "    # Tolerance settings\n",
    "    abs_tolerance = 1e-2  # Absolute tolerance (small for near-zero values)\n",
    "    rel_tolerance = 1e-3  # Relative tolerance (0.1% of the magnitude)\n",
    "    \n",
    "    # Check if differences are within tolerance\n",
    "    # Element-wise check: acceptable if diff <= abs_tolerance + rel_tolerance * magnitude\n",
    "    tolerance = abs_tolerance + rel_tolerance * magnitude\n",
    "    within_tolerance = torch.all(diff <= tolerance)\n",
    "    \n",
    "    if within_tolerance:\n",
    "        print(\"Results match within tolerance!\")\n",
    "    else:\n",
    "        print(\"Results differ beyond tolerance.\")\n",
    "    \n",
    "    return diff\n",
    "if __name__ == \"__main__\":\n",
    "    diff=test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04720854-5e5e-4f54-b081-6ac067c36a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9985, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00d443-9e66-4d09-8146-be25733ad073",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=33\n",
    "k& 0x0F,(k >> 4) & 0x0F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e39cff-e31e-4123-b86a-be3d372c0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,90):\n",
    "    print(i)\n",
    "    print(torch.max(diff[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb68cc-f5af-42e6-9a13-f308779ae213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
